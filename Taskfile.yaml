version: "3"

dotenv: [".env"]
silent: true  # Suppress command echoing by default (use --verbose to see commands)

vars:
  # Derived paths (computed from env vars)
  PENGUINS_CSV: "$DATA_DIR/penguins.csv"
  PENGUINS_PARQUET: "$DATA_DIR/penguins.parquet"
  # pg_lake
  PG_LAKE_DIR: ./work/pg_lake
  PG_LAKE_REPO: https://github.com/Snowflake-Labs/pg_lake.git
  PG_MAJOR: "18"

tasks:
  default:
    desc: Show available tasks
    cmds:
      - task --list

  # =============================================================================
  # Setup tasks (setup:*)
  # =============================================================================

  setup:aws:
    desc: Set up local AWS config for LocalStack
    cmds:
      - cp -r .aws.example .aws
      - echo "Created .aws/ directory with LocalStack config"
    status:
      - test -d .aws

  setup:env:
    desc: Create .env from .env.example
    cmds:
      - cp .env.example .env
      - echo "Created .env file"
    status:
      - test -f .env

  setup:all:
    desc: Run all setup tasks
    cmds:
      - task: setup:env
      - task: setup:aws
      - uv sync
      - direnv allow 2>/dev/null || true

  # =============================================================================
  # pg_lake tasks (pg_lake:*)
  # =============================================================================

  pg_lake:clone:
    desc: Clone pg_lake docker folder (sparse checkout)
    cmds:
      - rm -rf {{.PG_LAKE_DIR}}
      - git clone --filter=blob:none --no-checkout --depth 1 {{.PG_LAKE_REPO}} {{.PG_LAKE_DIR}}
      - |
        cd {{.PG_LAKE_DIR}}
        git sparse-checkout init --cone
        git sparse-checkout set docker
        git checkout
      - echo "Cloned pg_lake/docker to {{.PG_LAKE_DIR}}"
    status:
      - test -f {{.PG_LAKE_DIR}}/docker/Taskfile.yml

  pg_lake:up:
    desc: Start pg_lake services (PostgreSQL + LocalStack)
    deps: [pg_lake:clone]
    cmds:
      - cd {{.PG_LAKE_DIR}}/docker && task compose:up PG_MAJOR={{.PG_MAJOR}}

  pg_lake:down:
    desc: Stop pg_lake services
    cmds:
      - cd {{.PG_LAKE_DIR}}/docker && task compose:down PG_MAJOR={{.PG_MAJOR}}

  pg_lake:logs:
    desc: View pg_lake service logs
    cmds:
      - cd {{.PG_LAKE_DIR}}/docker && task compose:logs PG_MAJOR={{.PG_MAJOR}}

  pg_lake:teardown:
    desc: Stop pg_lake and remove volumes
    cmds:
      - cd {{.PG_LAKE_DIR}}/docker && task compose:teardown PG_MAJOR={{.PG_MAJOR}}

  # =============================================================================
  # Data tasks (data:*)
  # =============================================================================

  data:download:
    desc: Download penguins_cleaned.csv from GitHub
    cmds:
      - mkdir -p $DATA_DIR
      - curl -fsSL -o {{.PENGUINS_CSV}} $PENGUINS_CSV_URL
    sources:
      - Taskfile.yaml
    generates:
      - "{{.PENGUINS_CSV}}"

  data:convert:
    desc: Convert penguins CSV to Parquet format
    deps: [data:download]
    cmds:
      - uv run python scripts/csv_to_parquet.py {{.PENGUINS_CSV}} {{.PENGUINS_PARQUET}}
    sources:
      - "{{.PENGUINS_CSV}}"
      - scripts/csv_to_parquet.py
    generates:
      - "{{.PENGUINS_PARQUET}}"

  data:penguins:
    desc: Download penguins CSV and convert to Parquet
    cmds:
      - task: data:download
      - task: data:convert

  data:clean:
    desc: Remove downloaded and generated data files
    cmds:
      - rm -f {{.PENGUINS_CSV}} {{.PENGUINS_PARQUET}}

  # =============================================================================
  # S3 tasks (s3:*) - LocalStack
  # =============================================================================

  s3:create-bucket:
    desc: Create S3 bucket in LocalStack
    preconditions:
      - sh: curl -sf $AWS_ENDPOINT_URL/_localstack/health > /dev/null 2>&1
        msg: "LocalStack is not running. Run: task pg_lake:up"
    cmds:
      - uv run aws s3 mb s3://$S3_BUCKET 2>/dev/null || true
    status:
      - uv run aws s3 ls s3://$S3_BUCKET 2>/dev/null

  s3:upload:
    desc: Upload data files to LocalStack S3 bucket
    deps: [s3:create-bucket, data:convert]
    cmds:
      - uv run aws s3 cp {{.PENGUINS_CSV}} s3://$S3_BUCKET/raw/
      - uv run aws s3 cp {{.PENGUINS_PARQUET}} s3://$S3_BUCKET/$S3_PARQUET_KEY

  s3:list:
    desc: List files in LocalStack S3 bucket
    cmds:
      - uv run aws s3 ls s3://$S3_BUCKET/ --recursive

  s3:list-buckets:
    desc: List all S3 buckets in LocalStack
    cmds:
      - uv run aws s3 ls

  # =============================================================================
  # Health check tasks (check:*)
  # =============================================================================

  check:pg:
    desc: Check if pg_lake PostgreSQL is running
    cmds:
      - |
        if ! docker ps --format '{{"{{"}}.Names{{"}}"}}' | grep -q "^${PG_CONTAINER}$"; then
          echo "ERROR: Container '$PG_CONTAINER' is not running"
          echo ""
          echo "Start pg_lake first:"
          echo "  task pg_lake:up"
          exit 1
        fi
      - |
        if ! docker exec $PG_CONTAINER pg_isready -U $PGUSER -d $PGDATABASE -q; then
          echo "ERROR: PostgreSQL is not accepting connections"
          exit 1
        fi
        echo "PostgreSQL: OK"

  check:localstack:
    desc: Check if LocalStack is running
    cmds:
      - |
        if ! curl -sf $AWS_ENDPOINT_URL/_localstack/health > /dev/null 2>&1; then
          echo "ERROR: LocalStack is not running at $AWS_ENDPOINT_URL"
          echo ""
          echo "Start pg_lake first:"
          echo "  task pg_lake:up"
          exit 1
        fi
        echo "LocalStack: OK"

  check:s3:
    desc: Check if S3 operations work (create/delete test bucket)
    cmds:
      - |
        TEST_BUCKET="pg-lake-demo-health-check-$$"
        if ! uv run aws s3 mb s3://$TEST_BUCKET > /dev/null 2>&1; then
          echo "ERROR: Failed to create S3 bucket"
          exit 1
        fi
        if ! uv run aws s3 rb s3://$TEST_BUCKET > /dev/null 2>&1; then
          echo "ERROR: Failed to delete S3 bucket"
          exit 1
        fi
        echo "S3: OK"

  check:all:
    desc: Check if all required services are running
    cmds:
      - task: check:pg
      - task: check:localstack
      - task: check:s3

  # =============================================================================
  # SQL tasks (sql:*)
  # =============================================================================

  sql:run:
    desc: "Run SQL on PostgreSQL (Usage: task sql:run FILE=scripts/sql/01_init.sql)"
    preconditions:
      - sh: docker ps --format '{{"{{"}}.Names{{"}}"}}' | grep -q "^${PG_CONTAINER}$"
        msg: "pg_lake container is not running. Run: task pg_lake:up"
    cmds:
      - docker exec -i $PG_CONTAINER psql $PSQL_OPTS -U $PGUSER -d $PGDATABASE < {{.FILE}}

  sql:run-duckdb:
    desc: "Run SQL on DuckDB/pgduck_server (Usage: task sql:run-duckdb FILE=...)"
    preconditions:
      - sh: docker ps --format '{{"{{"}}.Names{{"}}"}}' | grep -q "^${PG_CONTAINER}$"
        msg: "pg_lake container is not running. Run: task pg_lake:up"
    cmds:
      - docker exec -i $PG_CONTAINER psql $PSQL_OPTS -h /home/postgres/pgduck_socket_dir -p 5332 -U $PGUSER < {{.FILE}}

  # =============================================================================
  # DuckDB tasks (duckdb:*)
  # =============================================================================

  duckdb:list-secrets:
    desc: List all DuckDB secrets
    preconditions:
      - sh: docker ps --format '{{"{{"}}.Names{{"}}"}}' | grep -q "^${PG_CONTAINER}$"
        msg: "pg_lake container is not running. Run: task pg_lake:up"
    cmds:
      - echo "SELECT name, type, scope FROM duckdb_secrets();" | docker exec -i $PG_CONTAINER psql $PSQL_OPTS -h /home/postgres/pgduck_socket_dir -p 5332 -U $PGUSER

  pg_lake:settings:
    desc: Show pg_lake configuration settings
    preconditions:
      - sh: docker ps --format '{{"{{"}}.Names{{"}}"}}' | grep -q "^${PG_CONTAINER}$"
        msg: "pg_lake container is not running. Run: task pg_lake:up"
    cmds:
      - |
        docker exec $PG_CONTAINER psql $PSQL_OPTS -U $PGUSER -d $PGDATABASE -c "
        SELECT name, setting, unit, short_desc 
        FROM pg_settings 
        WHERE name LIKE 'pg_lake%' 
        ORDER BY name;"

  # =============================================================================
  # Iceberg tasks (iceberg:*)
  # =============================================================================

  iceberg:list-tables:
    desc: List all Iceberg tables
    preconditions:
      - sh: docker ps --format '{{"{{"}}.Names{{"}}"}}' | grep -q "^${PG_CONTAINER}$"
        msg: "pg_lake container is not running. Run: task pg_lake:up"
    cmds:
      - |
        docker exec $PG_CONTAINER psql $PSQL_OPTS -U $PGUSER -d $PGDATABASE -c "
        SELECT catalog_name, table_namespace, table_name, metadata_location 
        FROM iceberg_tables;"

  iceberg:list-snapshots:
    desc: List Iceberg table snapshots (time travel history)
    preconditions:
      - sh: docker ps --format '{{"{{"}}.Names{{"}}"}}' | grep -q "^${PG_CONTAINER}$"
        msg: "pg_lake container is not running. Run: task pg_lake:up"
    vars:
      TABLE: '{{.TABLE | default "penguins_iceberg"}}'
    cmds:
      - printf '\033c' 2>/dev/null || true
      - |
        # Get metadata location from iceberg_tables, then query snapshots via DuckDB
        METADATA=$(docker exec $PG_CONTAINER psql -U $PGUSER -d $PGDATABASE -t -A -c \
          "SELECT metadata_location FROM iceberg_tables WHERE table_name = '{{.TABLE}}';")
        if [ -z "$METADATA" ]; then
          echo "ERROR: Table '{{.TABLE}}' not found in iceberg_tables"
          exit 1
        fi
        echo "=== ICEBERG SNAPSHOTS for {{.TABLE}} ==="
        echo ""
        docker exec $PG_CONTAINER psql -h /home/postgres/pgduck_socket_dir -p 5332 -U $PGUSER -c \
          "SELECT sequence_number as seq, snapshot_id, timestamp_ms as created_at FROM iceberg_snapshots('$METADATA') ORDER BY sequence_number;"

  iceberg:list-history:
    desc: List transaction history (metadata versions and deletion queue)
    preconditions:
      - sh: docker ps --format '{{"{{"}}.Names{{"}}"}}' | grep -q "^${PG_CONTAINER}$"
        msg: "pg_lake container is not running. Run: task pg_lake:up"
    cmds:
      - printf '\033c' 2>/dev/null || true
      - |
        echo "=== CURRENT & PREVIOUS METADATA ==="
        docker exec $PG_CONTAINER psql -U $PGUSER -d $PGDATABASE -c "
        SELECT 
            'Current' as version,
            regexp_replace(metadata_location, '.*/', '') as metadata_file
        FROM iceberg_tables WHERE table_name = 'penguins_iceberg'
        UNION ALL
        SELECT 
            'Previous' as version,
            regexp_replace(previous_metadata_location, '.*/', '') as metadata_file
        FROM iceberg_tables WHERE table_name = 'penguins_iceberg' AND previous_metadata_location IS NOT NULL;"
        echo ""
        echo "=== DELETION QUEUE (older versions, requires 2+ transactions) ==="
        docker exec $PG_CONTAINER psql -U $PGUSER -d $PGDATABASE -c "
        SELECT 
            d.orphaned_at as expired_at,
            CASE 
                WHEN extract(epoch from now() - d.orphaned_at) < 60 
                    THEN round(extract(epoch from now() - d.orphaned_at)) || ' sec ago'
                WHEN extract(epoch from now() - d.orphaned_at) < 3600 
                    THEN round(extract(epoch from now() - d.orphaned_at) / 60) || ' min ago'
                ELSE round(extract(epoch from now() - d.orphaned_at) / 3600, 1) || ' hours ago'
            END as age,
            regexp_replace(d.path, '.*/', '') as metadata_file
        FROM lake_engine.deletion_queue d
        JOIN iceberg_tables t ON d.path LIKE regexp_replace(t.metadata_location, '/metadata/.*', '') || '/%'
        WHERE t.table_name = 'penguins_iceberg' AND d.path LIKE '%.metadata.json'
        ORDER BY d.orphaned_at DESC LIMIT 5;"

  # =============================================================================
  # Demo tasks (demo:*)
  # =============================================================================

  demo:secret:
    desc: Create wildlife S3 secret for $S3_BUCKET bucket
    preconditions:
      - sh: docker ps --format '{{"{{"}}.Names{{"}}"}}' | grep -q "^${PG_CONTAINER}$"
        msg: "pg_lake container is not running. Run: task pg_lake:up"
    cmds:
      - printf '\033c' 2>/dev/null || true
      - envsubst < scripts/sql/00_create_secret.sql | docker exec -i $PG_CONTAINER psql $PSQL_OPTS -h /home/postgres/pgduck_socket_dir -p 5332 -U $PGUSER

  demo:init:
    desc: Initialize pg_lake extension and set default bucket
    preconditions:
      - sh: docker ps --format '{{"{{"}}.Names{{"}}"}}' | grep -q "^${PG_CONTAINER}$"
        msg: "pg_lake container is not running. Run: task pg_lake:up"
    cmds:
      - printf '\033c' 2>/dev/null || true
      - envsubst < scripts/sql/01_init.sql | docker exec -i $PG_CONTAINER psql $PSQL_OPTS -U $PGUSER -d $PGDATABASE

  demo:foreign-table:
    desc: "Part 1 - Create foreign table for Parquet (schema inference)"
    preconditions:
      - sh: docker ps --format '{{"{{"}}.Names{{"}}"}}' | grep -q "^${PG_CONTAINER}$"
        msg: "pg_lake container is not running. Run: task pg_lake:up"
    cmds:
      - printf '\033c' 2>/dev/null || true
      - envsubst < scripts/sql/02_create_foreign_table.sql | docker exec -i $PG_CONTAINER psql $PSQL_OPTS -U $PGUSER -d $PGDATABASE

  demo:iceberg:
    desc: "Part 2 - Upgrade to Iceberg table"
    preconditions:
      - sh: docker ps --format '{{"{{"}}.Names{{"}}"}}' | grep -q "^${PG_CONTAINER}$"
        msg: "pg_lake container is not running. Run: task pg_lake:up"
    cmds:
      - printf '\033c' 2>/dev/null || true
      - envsubst < scripts/sql/03_upgrade_to_iceberg.sql | docker exec -i $PG_CONTAINER psql $PSQL_OPTS -U $PGUSER -d $PGDATABASE

  demo:modify:
    desc: "Part 3 - ACID operations (DELETE)"
    preconditions:
      - sh: docker ps --format '{{"{{"}}.Names{{"}}"}}' | grep -q "^${PG_CONTAINER}$"
        msg: "pg_lake container is not running. Run: task pg_lake:up"
    cmds:
      - printf '\033c' 2>/dev/null || true
      - docker exec -i $PG_CONTAINER psql $PSQL_OPTS -U $PGUSER -d $PGDATABASE < scripts/sql/04_modify_iceberg.sql

  demo:time-travel:
    desc: "Part 4 - Time travel to see deleted data"
    preconditions:
      - sh: docker ps --format '{{"{{"}}.Names{{"}}"}}' | grep -q "^${PG_CONTAINER}$"
        msg: "pg_lake container is not running. Run: task pg_lake:up"
    cmds:
      - printf '\033c' 2>/dev/null || true
      # No PSQL_OPTS here - cleaner output without query echo
      - envsubst < scripts/sql/05_time_travel.sql | docker exec -i $PG_CONTAINER psql -U $PGUSER -d $PGDATABASE

  demo:export:
    desc: "Part 5 - Export to S3 (COPY TO)"
    preconditions:
      - sh: docker ps --format '{{"{{"}}.Names{{"}}"}}' | grep -q "^${PG_CONTAINER}$"
        msg: "pg_lake container is not running. Run: task pg_lake:up"
    cmds:
      - printf '\033c' 2>/dev/null || true
      - envsubst < scripts/sql/06_export_to_s3.sql | docker exec -i $PG_CONTAINER psql $PSQL_OPTS -U $PGUSER -d $PGDATABASE

  demo:reset:
    desc: Reset demo to clean state (can restart from demo:init)
    preconditions:
      - sh: docker ps --format '{{"{{"}}.Names{{"}}"}}' | grep -q "^${PG_CONTAINER}$"
        msg: "pg_lake container is not running. Run: task pg_lake:up"
    cmds:
      # Drop tables and extension
      - |
        docker exec $PG_CONTAINER psql $PSQL_OPTS -U $PGUSER -d $PGDATABASE -c "
        DROP TABLE IF EXISTS penguins_iceberg;
        DROP FOREIGN TABLE IF EXISTS penguins_raw;
        DROP FOREIGN TABLE IF EXISTS penguins_time_travel;
        DROP EXTENSION IF EXISTS pg_lake CASCADE;"
      # Clear S3 bucket and recreate empty
      - uv run aws s3 rb s3://$S3_BUCKET --force 2>/dev/null || true
      - uv run aws s3 mb s3://$S3_BUCKET
      # Re-upload data
      - task: s3:upload

  demo:all:
    desc: Run full demo sequence
    cmds:
      - task: check:all
      - task: demo:secret
      - task: demo:init
      - task: demo:foreign-table
      - task: demo:iceberg
      - task: demo:modify
      - task: demo:time-travel
      - task: demo:export

  demo:teardown:
    desc: Full teardown (reset + stop containers + remove volumes)
    cmds:
      - task: pg_lake:teardown
      - task: data:clean
